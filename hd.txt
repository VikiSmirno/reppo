Этап 1. Разведочный анализ (EDA) и первичная гигиена данных

Цель: Понять природу данных и устранить технические ошибки формата.

Загрузка и аудит структуры: Оценка размерности (shape), типов данных (info) и просмотр примеров (head).

Поиск скрытых пропусков: Выявление нестандартных заполнителей (например, '-', 'Не определено', пустые строки) и их конвертация в стандартный формат NaN.

Приведение типов: Принудительное преобразование числовых признаков, ошибочно распознанных как строки (object 
→
→
 float).

Анализ кардинальности: Исследование уникальных значений для выявления категориальных признаков и их кодирование (Mapping/Encoding).

Этап 2. Конструирование признаков (Feature Engineering)

Цель: Создать новые метрики, которые лучше описывают предметную область, пока данные еще "сырые".

Генерация экспертных признаков: Создание новых переменных на основе физического смысла (суммы, отношения, эффективность, удельные показатели).

Удаление идентификаторов: Исключение признаков, не несущих предсказательной силы (ID, номера записей).

Этап 3. Продвинутая обработка пропущенных значений

Цель: Восстановить потерянную информацию научно обоснованным методом, минимизируя вносимые искажения.

Диагностика пропусков: Оценка процента потерь. Удаление признаков с критическим объемом пропусков (>70-80%), если их восстановление невозможно.

Сравнение стратегий (Imputation Strategy):

Статистические методы: Среднее, медиана, мода.

Машинные методы: KNN (ближайшие соседи), MICE (итеративная регрессия).

Валидация методов:

KL-дивергенция: Оценка того, насколько изменилось распределение данных после заполнения.

NRMSE (на отложенной выборке): Проведение эксперимента с искусственным удалением данных (Ground Truth) для математического выбора лучшего алгоритма.

Финальное заполнение: Применение победившего метода (в данном случае KNN) ко всему датасету.

Этап 4. Двухуровневая очистка от выбросов

Цель: Убрать шум и аномалии, которые могут сбить модель с толку.

Одномерный анализ (Univariate): Поиск выбросов в отдельных столбцах с использованием стратегии пересечения («Консенсус»):

Метод межквартильного размаха (IQR).

Робастный Z-score (через MAD).

Действие: Удаление записи, только если оба метода считают её выбросом.

Многомерный анализ (Multivariate):

Снижение размерности (UMAP): Визуализация данных в 2D для поиска оторванных кластеров.

Ансамбль алгоритмов: Использование Isolation Forest, One-Class SVM и DBSCAN.

Действие: Удаление точек, признанных аномальными большинством или всеми алгоритмами одновременно.

Этап 5. Отбор признаков и масштабирование

Цель: Устранить дублирование информации и подготовить данные для математических алгоритмов.

Борьба с мультиколлинеарностью: Построение матрицы корреляций. Удаление признаков, имеющих корреляцию выше порога (например, 
>0.9
), так как они дублируют друг друга.

Масштабирование (Scaling): Применение устойчивых методов (например, RobustScaler), так как данные могут содержать остаточные "тяжелые хвосты".

Анализ главных компонент (PCA): Попытка сжатия пространства признаков. Оценка корреляции компонент с целевой переменной (если корреляция низкая, лучше использовать отобранные исходные признаки).

Этап 6. Машинное обучение и интерпретация

Цель: Найти оптимальную модель и объяснить её решения.

Выбор пространства моделей (Model Zoo): Тестирование алгоритмов разной природы:

Линейные (Ridge, SVR) — для проверки простых гипотез.

Ансамблевые (RandomForest, Gradient Boosting/LGBM) — для поиска сложных нелинейных зависимостей.

Байесовская оптимизация (Optuna): Умный подбор гиперпараметров с кросс-валидацией, вместо простого перебора.

Оценка качества: Использование метрик R2(коэффициент детерминации) и MSE/RMSE. Сравнение предсказаний с фактом (Scatter plot).

Интерпретация (XAI): Использование SHAP-values для ранжирования признаков по важности и понимания направления их влияния на целевую переменную.